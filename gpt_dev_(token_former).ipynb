{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rajarshi12321/gpt-dev-token-former-/blob/main/gpt_dev_(token_former).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a GPT\n",
        "\n",
        "Companion notebook to the [Zero To Hero](https://karpathy.ai/zero-to-hero.html) video on GPT."
      ],
      "metadata": {
        "id": "wJpXpmjEYC_T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5hjCcLDr2WC",
        "outputId": "152b3f47-e5d1-4bb2-9051-0b50fcc8a4ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-28 15:07:18--  https://raw.githubusercontent.com/Rajarshi12321/gpt-dev-token-former-/refs/heads/main/pokemon.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 509985 (498K) [text/plain]\n",
            "Saving to: ‘pokemon.txt’\n",
            "\n",
            "pokemon.txt         100%[===================>] 498.03K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-11-28 15:07:19 (10.1 MB/s) - ‘pokemon.txt’ saved [509985/509985]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
        "!wget https://raw.githubusercontent.com/Rajarshi12321/gpt-dev-token-former-/refs/heads/main/pokemon.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read it in to inspect it\n",
        "with open('pokemon.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "O6medjfRsLD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xWI_VyAsN8F",
        "outputId": "73917011-d454-45f4-a8de-61c0e770b6ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  509245\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's look at the first 1000 characters\n",
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c5V0FvqseE0",
        "outputId": "49d5fb0e-8f07-4407-8cba-44c7c869800b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---------------------Chaper - 1--------------------\n",
            "\n",
            "Ash Ketchum: Being More\n",
            "VERY IMPORTANT NOTICE: This is a repost of a story that I deleted quite some time ago, as I lost interest in it, but as I am sometimes still receiving PMs telling me that people miss it, I am reposting it for those who are still interested in reading it and for posterity's sake. Be warned, though. I have no intention to work on this story any time soon; only if and when my interest for this story returns will I work on it, and that is a very big if.\n",
            "\n",
            "I will post the other finished chapters of this story in the coming week. I noticed that the grammer isn't up to my standards anymore as I have progressed as a writer, and that annoys me something fierce, so I will first fix that before I post the rest.\n",
            "\n",
            "Anyway, enjoy.\n",
            "\n",
            "Ash Ketchum: Being More.\n",
            "\n",
            "Becoming More.\n",
            "\n",
            "Seeing the world at an early age radically changed the boy known as Ash Ketchum.\n",
            "\n",
            "Ash Ketchum's mother, Delia Ketchum, loved to travel almost as much as s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e-Rbyr8sfM8",
        "outputId": "776818c3-e3c7-4d9a-c3e2-bcc9e0d4282c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !\"'(),-./1234789:;?ABCDEFGHIJKLMNOPQRSTUVWYZabcdefghijklmnopqrstuvwxyzàéó–…\n",
            "77\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yw1LKNCgwjj1",
        "outputId": "7b7b8b81-57cc-4e1f-bfaf-ed98ff0e94e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[53, 54, 54, 1, 65, 53, 50, 63, 50]\n",
            "hii there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
        "import torch # we use PyTorch: https://pytorch.org\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJb0OXPwzvqg",
        "outputId": "dd1368b7-bce3-47f6-cd38-81ae48bd3514"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([509245]) torch.int64\n",
            "tensor([ 0,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
            "         8,  8,  8,  8, 23, 53, 46, 61, 50, 63,  1,  8,  1, 11,  8,  8,  8,  8,\n",
            "         8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  0,  0,\n",
            "        21, 64, 53,  1, 31, 50, 65, 48, 53, 66, 58, 18,  1, 22, 50, 54, 59, 52,\n",
            "         1, 33, 60, 63, 50,  0, 42, 25, 38, 44,  1, 29, 33, 36, 35, 38, 40, 21,\n",
            "        34, 40,  1, 34, 35, 40, 29, 23, 25, 18,  1, 40, 53, 54, 64,  1, 54, 64,\n",
            "         1, 46,  1, 63, 50, 61, 60, 64, 65,  1, 60, 51,  1, 46,  1, 64, 65, 60,\n",
            "        63, 70,  1, 65, 53, 46, 65,  1, 29,  1, 49, 50, 57, 50, 65, 50, 49,  1,\n",
            "        62, 66, 54, 65, 50,  1, 64, 60, 58, 50,  1, 65, 54, 58, 50,  1, 46, 52,\n",
            "        60,  7,  1, 46, 64,  1, 29,  1, 57, 60, 64, 65,  1, 54, 59, 65, 50, 63,\n",
            "        50, 64, 65,  1, 54, 59,  1, 54, 65,  7,  1, 47, 66, 65,  1, 46, 64,  1,\n",
            "        29,  1, 46, 58,  1, 64, 60, 58, 50, 65, 54, 58, 50, 64,  1, 64, 65, 54,\n",
            "        57, 57,  1, 63, 50, 48, 50, 54, 67, 54, 59, 52,  1, 36, 33, 64,  1, 65,\n",
            "        50, 57, 57, 54, 59, 52,  1, 58, 50,  1, 65, 53, 46, 65,  1, 61, 50, 60,\n",
            "        61, 57, 50,  1, 58, 54, 64, 64,  1, 54, 65,  7,  1, 29,  1, 46, 58,  1,\n",
            "        63, 50, 61, 60, 64, 65, 54, 59, 52,  1, 54, 65,  1, 51, 60, 63,  1, 65,\n",
            "        53, 60, 64, 50,  1, 68, 53, 60,  1, 46, 63, 50,  1, 64, 65, 54, 57, 57,\n",
            "         1, 54, 59, 65, 50, 63, 50, 64, 65, 50, 49,  1, 54, 59,  1, 63, 50, 46,\n",
            "        49, 54, 59, 52,  1, 54, 65,  1, 46, 59, 49,  1, 51, 60, 63,  1, 61, 60,\n",
            "        64, 65, 50, 63, 54, 65, 70,  4, 64,  1, 64, 46, 56, 50,  9,  1, 22, 50,\n",
            "         1, 68, 46, 63, 59, 50, 49,  7,  1, 65, 53, 60, 66, 52, 53,  9,  1, 29,\n",
            "         1, 53, 46, 67, 50,  1, 59, 60,  1, 54, 59, 65, 50, 59, 65, 54, 60, 59,\n",
            "         1, 65, 60,  1, 68, 60, 63, 56,  1, 60, 59,  1, 65, 53, 54, 64,  1, 64,\n",
            "        65, 60, 63, 70,  1, 46, 59, 70,  1, 65, 54, 58, 50,  1, 64, 60, 60, 59,\n",
            "        19,  1, 60, 59, 57, 70,  1, 54, 51,  1, 46, 59, 49,  1, 68, 53, 50, 59,\n",
            "         1, 58, 70,  1, 54, 59, 65, 50, 63, 50, 64, 65,  1, 51, 60, 63,  1, 65,\n",
            "        53, 54, 64,  1, 64, 65, 60, 63, 70,  1, 63, 50, 65, 66, 63, 59, 64,  1,\n",
            "        68, 54, 57, 57,  1, 29,  1, 68, 60, 63, 56,  1, 60, 59,  1, 54, 65,  7,\n",
            "         1, 46, 59, 49,  1, 65, 53, 46, 65,  1, 54, 64,  1, 46,  1, 67, 50, 63,\n",
            "        70,  1, 47, 54, 52,  1, 54, 51,  9,  0,  0, 29,  1, 68, 54, 57, 57,  1,\n",
            "        61, 60, 64, 65,  1, 65, 53, 50,  1, 60, 65, 53, 50, 63,  1, 51, 54, 59,\n",
            "        54, 64, 53, 50, 49,  1, 48, 53, 46, 61, 65, 50, 63, 64,  1, 60, 51,  1,\n",
            "        65, 53, 54, 64,  1, 64, 65, 60, 63, 70,  1, 54, 59,  1, 65, 53, 50,  1,\n",
            "        48, 60, 58, 54, 59, 52,  1, 68, 50, 50, 56,  9,  1, 29,  1, 59, 60, 65,\n",
            "        54, 48, 50, 49,  1, 65, 53, 46, 65,  1, 65, 53, 50,  1, 52, 63, 46, 58,\n",
            "        58, 50, 63,  1, 54, 64, 59,  4, 65,  1, 66, 61,  1, 65, 60,  1, 58, 70,\n",
            "         1, 64, 65, 46, 59, 49, 46, 63, 49, 64,  1, 46, 59, 70, 58, 60, 63, 50,\n",
            "         1, 46, 64,  1, 29,  1, 53, 46, 67, 50,  1, 61, 63, 60, 52, 63, 50, 64,\n",
            "        64, 50, 49,  1, 46, 64,  1, 46,  1, 68, 63, 54, 65, 50, 63,  7,  1, 46,\n",
            "        59, 49,  1, 65, 53, 46, 65,  1, 46, 59, 59, 60, 70, 64,  1, 58, 50,  1,\n",
            "        64, 60, 58, 50, 65, 53, 54, 59, 52,  1, 51, 54, 50, 63, 48, 50,  7,  1,\n",
            "        64, 60,  1, 29,  1, 68, 54, 57, 57,  1, 51, 54, 63, 64, 65,  1, 51, 54,\n",
            "        69,  1, 65, 53, 46, 65,  1, 47, 50, 51, 60, 63, 50,  1, 29,  1, 61, 60,\n",
            "        64, 65,  1, 65, 53, 50,  1, 63, 50, 64, 65,  9,  0,  0, 21, 59, 70, 68,\n",
            "        46, 70,  7,  1, 50, 59, 55, 60, 70,  9,  0,  0, 21, 64, 53,  1, 31, 50,\n",
            "        65, 48, 53, 66, 58, 18,  1, 22, 50, 54, 59, 52,  1, 33, 60, 63, 50,  9,\n",
            "         0,  0, 22, 50, 48, 60, 58, 54, 59, 52,  1, 33, 60, 63, 50,  9,  0,  0,\n",
            "        39, 50, 50, 54, 59, 52,  1, 65, 53, 50,  1, 68, 60, 63, 57, 49,  1, 46,\n",
            "        65,  1, 46, 59,  1, 50, 46, 63, 57, 70,  1, 46, 52, 50,  1, 63, 46, 49,\n",
            "        54, 48, 46, 57, 57, 70,  1, 48, 53, 46, 59, 52, 50, 49,  1, 65, 53, 50,\n",
            "         1, 47, 60, 70,  1, 56, 59, 60, 68, 59,  1, 46, 64,  1, 21, 64, 53,  1,\n",
            "        31, 50, 65, 48, 53, 66, 58,  9,  0,  0, 21, 64, 53,  1, 31, 50, 65, 48,\n",
            "        53, 66, 58,  4, 64,  1, 58, 60, 65, 53, 50, 63,  7,  1, 24, 50, 57, 54,\n",
            "        46,  1, 31, 50, 65, 48, 53, 66, 58,  7,  1, 57, 60, 67, 50, 49,  1, 65,\n",
            "        60,  1, 65, 63, 46, 67, 50, 57,  1, 46, 57, 58, 60, 64, 65,  1, 46, 64,\n",
            "         1, 58, 66, 48, 53,  1, 46, 64,  1, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's now split up the data into train and validation sets\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "f_WIXqxz0lU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TD5Bj8Y6IAD4",
        "outputId": "532d43a4-22cd-4460-80a6-d3e3bc37984e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 8, 8, 8, 8, 8, 8, 8, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HXDe8vGJCEn",
        "outputId": "b1e3e84b-3912-41af-9cf4-3a5ba1d56f84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([0]) the target: 8\n",
            "when input is tensor([0, 8]) the target: 8\n",
            "when input is tensor([0, 8, 8]) the target: 8\n",
            "when input is tensor([0, 8, 8, 8]) the target: 8\n",
            "when input is tensor([0, 8, 8, 8, 8]) the target: 8\n",
            "when input is tensor([0, 8, 8, 8, 8, 8]) the target: 8\n",
            "when input is tensor([0, 8, 8, 8, 8, 8, 8]) the target: 8\n",
            "when input is tensor([0, 8, 8, 8, 8, 8, 8, 8]) the target: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3k1Czf7LuA9",
        "outputId": "563be675-a19f-4ae6-8c6f-7bda05b741fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[53, 50, 63, 50,  1, 32, 66, 48],\n",
            "        [40, 53, 50, 59,  1, 65, 53, 46],\n",
            "        [50, 63, 60, 59,  1, 36, 46, 57],\n",
            "        [66, 52, 53, 65,  1, 60, 51,  1]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[50, 63, 50,  1, 32, 66, 48, 46],\n",
            "        [53, 50, 59,  1, 65, 53, 46, 65],\n",
            "        [63, 60, 59,  1, 36, 46, 57, 46],\n",
            "        [52, 53, 65,  1, 60, 51,  1, 64]])\n",
            "----\n",
            "when input is [53] the target: 50\n",
            "when input is [53, 50] the target: 63\n",
            "when input is [53, 50, 63] the target: 50\n",
            "when input is [53, 50, 63, 50] the target: 1\n",
            "when input is [53, 50, 63, 50, 1] the target: 32\n",
            "when input is [53, 50, 63, 50, 1, 32] the target: 66\n",
            "when input is [53, 50, 63, 50, 1, 32, 66] the target: 48\n",
            "when input is [53, 50, 63, 50, 1, 32, 66, 48] the target: 46\n",
            "when input is [40] the target: 53\n",
            "when input is [40, 53] the target: 50\n",
            "when input is [40, 53, 50] the target: 59\n",
            "when input is [40, 53, 50, 59] the target: 1\n",
            "when input is [40, 53, 50, 59, 1] the target: 65\n",
            "when input is [40, 53, 50, 59, 1, 65] the target: 53\n",
            "when input is [40, 53, 50, 59, 1, 65, 53] the target: 46\n",
            "when input is [40, 53, 50, 59, 1, 65, 53, 46] the target: 65\n",
            "when input is [50] the target: 63\n",
            "when input is [50, 63] the target: 60\n",
            "when input is [50, 63, 60] the target: 59\n",
            "when input is [50, 63, 60, 59] the target: 1\n",
            "when input is [50, 63, 60, 59, 1] the target: 36\n",
            "when input is [50, 63, 60, 59, 1, 36] the target: 46\n",
            "when input is [50, 63, 60, 59, 1, 36, 46] the target: 57\n",
            "when input is [50, 63, 60, 59, 1, 36, 46, 57] the target: 46\n",
            "when input is [66] the target: 52\n",
            "when input is [66, 52] the target: 53\n",
            "when input is [66, 52, 53] the target: 65\n",
            "when input is [66, 52, 53, 65] the target: 1\n",
            "when input is [66, 52, 53, 65, 1] the target: 60\n",
            "when input is [66, 52, 53, 65, 1, 60] the target: 51\n",
            "when input is [66, 52, 53, 65, 1, 60, 51] the target: 1\n",
            "when input is [66, 52, 53, 65, 1, 60, 51, 1] the target: 64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(xb) # our input to the transformer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpyyAeIzQjlO",
        "outputId": "3f6a9921-4b5b-4b39-df4a-dd5f2d7206eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[53, 50, 63, 50,  1, 32, 66, 48],\n",
            "        [40, 53, 50, 59,  1, 65, 53, 46],\n",
            "        [50, 63, 60, 59,  1, 36, 46, 57],\n",
            "        [66, 52, 53, 65,  1, 60, 51,  1]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nql_1ER53oCf",
        "outputId": "f3793ef9-a2da-4342-c039-7d77f42de02f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 77])\n",
            "tensor(4.8359, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "h9ydM \n",
            "7I9àe?.;K8uw?;Zh3Zi\"Kk!c!–BM'kv,kL!ITt'kG)y7y;\n",
            "q4tgb?:Q))O\n",
            "y8TOO'nm2 tFLseg…?gqHmjE;w1hIUFw t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "eTyJ8qAaDdiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "for steps in range(100): # increase number of steps for good results...\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs4kI8YdEkQj",
        "outputId": "9ea9ab89-7351-4629-9440-ddee3d2a699f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.480553150177002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcVIDWAZEtjN",
        "outputId": "bc2a3873-6981-4651-c256-9cd86905df1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "mPézaTàMsk8SvCDE–éUBq \"z/u,\"MG1VjmC-d-FFvVQR/sejt?.;ZKPOàIH7synEé4S,Txx!\"?heVswkvV\n",
            "àqa3BQ)wtQU\",2qq7é–)VBTJ W(PG1(n\" ahWFJH-Kh….R:xrACFwPSNóz/sYTfW44'1rSaTrfM;wEosM,H!Z8PSg\n",
            "rshiZ2ólkyO/TgP;!LZmRiJftgs–'2ziFAl!T)aTBb?tcWEéwt7L9'7s:L? wGrDOLTJiJV7QCDnéDx:.J(jpóE\n",
            "àPOgCYH\n",
            "h/q.NmbD/gbéfjqLYa\n",
            "oI9-dY:L\"cólreMfMzól?3kHdPMo/\n",
            "hTWViEFb\")q…j?Gkekmh1FH…Cz.misMA …i)f,EYG8dP.UjqyNRW \n",
            "cCfz?sj4bEg:FV C–CéfcàZuaGrórNF–eO4dxh/(3/)fj-3/t9o/zsYIvVejEYVpwdxCYreeOy,AdWE7)E–t2ZR4Pk1KoK8HHG\"VBqiGSaFbFGéTrZv.()!ój,YZejp–\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The mathematical trick in self-attention"
      ],
      "metadata": {
        "id": "XinV8nmAnmKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tukiH-NbRBhA",
        "outputId": "2723bb40-e12f-4cb1-99f2-095775272b6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# consider the following toy example:\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs_E24uRE8kr",
        "outputId": "dd81f1e0-1f1f-4e97-d9e8-d019f7185ccb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b,:t+1] # (t,C)\n",
        "        xbow[b,t] = torch.mean(xprev, 0)\n"
      ],
      "metadata": {
        "id": "86NuXX0fn7ps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# version 2: using matrix multiply for a weighted aggregation\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
        "torch.allclose(xbow, xbow2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhdOAd6-wXkZ",
        "outputId": "ab83d322-3b56-4830-90d3-bdd73ae3a29e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 3: use Softmax\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOURrfG-ysoL",
        "outputId": "090b5496-8380-4de5-bcf1-2d971f0658b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1lmQvpdwINeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kKxOjWZ3INRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7jvPzUT9HRj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HwxSC-lLHRVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# version 4: self-attention!\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "head_size = 16\n",
        "\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "\n",
        "k = key(x)   # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "#out = wei @ x\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDarxEWIRMKq",
        "outputId": "cc76134e-d7ba-4c18-bc9c-cfe2085cbd97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Lo-YT60csL9",
        "outputId": "1493171f-304d-48ac-f345-fe16e27e5515"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 5: Using Pattention - Implementing token parameter attention to better the self attention and easier scaling of models\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def pattention(X, input_dim, output_dim, scale_factor=None, KP=None, VP=None):\n",
        "\n",
        "    # Initialize or reuse the key projection layer\n",
        "    if KP is None:\n",
        "        KP = nn.Linear(input_dim, output_dim, bias=False)  # KP.weight: (num_params, input_dim)\n",
        "\n",
        "    # Initialize or reuse the value projection layer\n",
        "    if VP is None:\n",
        "        VP = nn.Linear(input_dim, output_dim, bias=False)  # VP.weight: (output_dim, num_params)\n",
        "\n",
        "    # Default scale factor if not provided\n",
        "    scale_factor = scale_factor or output_dim  # τ is set to n by default\n",
        "\n",
        "    # Compute attention scores (dot product of input with keys)\n",
        "    # X has shape (B, T, input_dim), and KP.weight has shape (num_params, input_dim)\n",
        "    A = torch.matmul(X, KP.weight.T)  # A will have shape (B, T, num_params)\n",
        "\n",
        "    # Apply modified softmax (normalized with sum of squared norms of each row)\n",
        "    norm_A_sq = torch.norm(A, p=2, dim=-1, keepdim=True) ** 2  # norm_A_sq: (B, T, 1)\n",
        "    S = (A * scale_factor) / (norm_A_sq + 1e-6)  # S will have shape (B, T, num_params)\n",
        "\n",
        "    # Apply GeLU non-linearity\n",
        "    S = F.gelu(S)  # Apply GeLU non-linearity to the scores, S shape: (B, T, num_params)\n",
        "\n",
        "    # Compute output using value projection\n",
        "    O = VP(S)  # Output shape: (B, T, output_dim)\n",
        "\n",
        "    return O\n",
        "\n",
        "\n",
        "# Test the function\n",
        "torch.manual_seed(1337)\n",
        "B, T, C = 4, 8, 32  # batch (B), time (T), channels (C)\n",
        "\n",
        "x = torch.randn(B, T, C)  # Input tensor X with shape (B, T, C)\n",
        "output_dim = 32  # Output dimension (output_dim)\n",
        "\n",
        "# Call pattention for key, query, and value\n",
        "k = pattention(x, C, output_dim)  # k shape: (B, T, output_dim)\n",
        "q = pattention(x, C, output_dim)  # q shape: (B, T, output_dim)\n",
        "v = pattention(x, C, output_dim)  # v shape: (B, T, output_dim)\n",
        "\n",
        "\n",
        "# Attention computation (scaled dot product)\n",
        "wei =  q @ k.transpose(-2, -1)  # wei shape: (B, T, T) -> (B, T, num_params) @ (B, num_params, T) ---> (B, T, T)\n",
        "\n",
        "# Apply mask (lower triangular mask for causal attention)\n",
        "tril = torch.tril(torch.ones(T, T))  # tril shape: (T, T)\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))  # wei shape: (B, T, T)\n",
        "\n",
        "# Apply softmax to obtain attention weights\n",
        "wei = F.softmax(wei, dim=-1)  # wei shape: (B, T, T)\n",
        "\n",
        "# Compute the final output (weighted sum of value vectors)\n",
        "out = wei @ v  # out shape: (B, T, output_dim)\n",
        "\n",
        "out.shape  # The shape of the output tensor (B, T, output_dim)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "Ilajon7mcsGa",
        "outputId": "30514cab-3d6d-45dd-b4e9-e8f3a050c48d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (32x32 and 16x32)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-a46e6e2f7520>\u001b[0m in \u001b[0;36m<cell line: 45>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# Call pattention for key, query, and value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# k shape: (B, T, output_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# q shape: (B, T, output_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# v shape: (B, T, output_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-65-a46e6e2f7520>\u001b[0m in \u001b[0;36mpattention\u001b[0;34m(X, input_dim, output_dim, scale_factor, KP, VP)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Compute attention scores (dot product of input with keys)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# X has shape (B, T, input_dim), and KP.weight has shape (num_params, input_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# A will have shape (B, T, num_params)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Apply modified softmax (normalized with sum of squared norms of each row)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x32 and 16x32)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vT1hdtzXCjgL",
        "outputId": "65dedda9-d7d8-45f7-cf57-08819090d8c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1700, 0.8300, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3983, 0.3845, 0.2172, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0313, 0.0182, 0.0644, 0.8861, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2203, 0.4824, 0.1029, 0.0101, 0.1842, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0068, 0.0063, 0.1457, 0.1020, 0.6764, 0.0628, 0.0000, 0.0000],\n",
              "        [0.2563, 0.4131, 0.1467, 0.0018, 0.1255, 0.0286, 0.0280, 0.0000],\n",
              "        [0.3788, 0.0443, 0.0818, 0.1685, 0.0434, 0.0681, 0.1810, 0.0342]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes:\n",
        "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
        "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
        "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
        "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
        "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
        "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
      ],
      "metadata": {
        "id": "M5CvobiQ0pLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = torch.randn(B,T,head_size)\n",
        "q = torch.randn(B,T,head_size)\n",
        "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
      ],
      "metadata": {
        "id": "4SNbLq5z3oBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nl6I9n9IRTSo",
        "outputId": "7e325e27-d94c-45f8-bfbe-a335381c015d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0037)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1tQx7oeRvtc",
        "outputId": "fab99047-c8c7-4ae5-babc-51943e80b662"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0966)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLb_odHU3iKM",
        "outputId": "d6e26c86-d68e-4257-a124-905d4def0448"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.2769)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JB82yzt44REI",
        "outputId": "8aedf7ce-8e1f-4172-e34d-e8c8a90c8aca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mpt8569BB9_f",
        "outputId": "2cd6f539-38fb-442b-cdbb-e38334b3b524"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm1d: # (used to be BatchNorm1d)\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    xmean = x.mean(1, keepdim=True) # batch mean\n",
        "    xvar = x.var(1, keepdim=True) # batch variance\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "module = LayerNorm1d(100)\n",
        "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
        "x = module(x)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Num7sX9CKOH",
        "outputId": "5ebd7ac3-d715-4ecd-941e-5332f840549c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "633T2cmnW1uk",
        "outputId": "f893832e-efaf-438f-ab50-f251fcfa635d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.1469), tensor(0.8803))"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LN9cK9BoXCYb",
        "outputId": "3b1c7fab-0554-4821-e28d-da56354752d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(-9.5367e-09), tensor(1.0000))"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# French to English translation example:\n",
        "\n",
        "# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n",
        "# les réseaux de neurones sont géniaux! <START> neural networks are awesome!<END>\n",
        "\n"
      ],
      "metadata": {
        "id": "dRJH6wM_XFfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The main updationtion -> Using Pattention block"
      ],
      "metadata": {
        "id": "emvxplzNb1n4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# version 5: Pattention\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Set manual seed for reproducibility\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# Batch size (B), sequence length (T), and input dimension (C)\n",
        "B, T, C = 4, 8, 32  # batch size, time steps, channels\n",
        "\n",
        "# Define number of learnable parameter tokens (n) and output dimension\n",
        "num_params = 16\n",
        "output_dim = 32\n",
        "\n",
        "# Input tensor (X) with shape (B, T, C)\n",
        "X = torch.randn(B, T, C)\n",
        "\n",
        "# Initialize Key Projection (KP) and Value Projection (VP) layers\n",
        "KP = nn.Linear(C, num_params, bias=False)\n",
        "VP = nn.Linear(num_params, output_dim, bias=False)\n",
        "\n",
        "# Compute attention scores (dot product of input with keys)\n",
        "# X has shape (B, T, C), and KP.weight has shape (num_params, C)\n",
        "A = torch.matmul(X, KP.weight.T)  # A will have shape (B, T, num_params)\n",
        "\n",
        "# Apply modified softmax (normalized with sum of squared norms of each row)\n",
        "norm_A_sq = torch.norm(A, p=2, dim=-1, keepdim=True) ** 2  # Shape: (B, T, 1)\n",
        "scale_factor = num_params  # Scaling factor (τ) is set to n by default\n",
        "S = (A * scale_factor) / (norm_A_sq + 1e-6)  # Modified softmax formula\n",
        "\n",
        "# Apply GeLU non-linearity to the scores\n",
        "S = F.gelu(S)  # Apply GeLU non-linearity to the attention scores\n",
        "\n",
        "# Compute output using value projection\n",
        "O = VP(S)  # Shape: (B, T, output_dim)\n",
        "\n",
        "# Print the shapes of the key, query, and value\n",
        "print(\"Key shape:\", A.shape)  # A is the attention score, representing the key\n",
        "print(\"Query shape:\", A.shape)  # Same as key, for this case query will be similar\n",
        "print(\"Value shape:\", O.shape)  # Output from the value projection\n",
        "\n",
        "# Optionally, print values to see the computed tensors\n",
        "# print(\"Key --> :\", A)\n",
        "# print(\"Query --> :\", A)\n",
        "# print(\"Value --> :\", O)\n"
      ],
      "metadata": {
        "id": "P50TDcQ0ZnRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class pattention:\n",
        "    def __init__(self, input_dim, output_dim, scale_factor=None, KP=None, VP=None):\n",
        "        \"\"\"\n",
        "        Initializes the attention mechanism with key and value projections.\n",
        "        \"\"\"\n",
        "\n",
        "        # Initialize or reuse the key projection layer (input_dim -> output_dim)\n",
        "        self.KP = KP if KP is not None else torch.randn(input_dim, output_dim, requires_grad=True)  # (input_dim, output_dim)\n",
        "\n",
        "        # Initialize or reuse the value projection layer (input_dim -> output_dim)\n",
        "        self.VP = VP if VP is not None else torch.randn(input_dim, output_dim, requires_grad=True)  # (input_dim, output_dim)\n",
        "\n",
        "        # Optional scale factor (defaults to output_dim if not provided)\n",
        "        self.scale_factor = scale_factor or output_dim\n",
        "\n",
        "    def forward(self, X):\n",
        "\n",
        "        # Compute the attention scores (dot product between input and key projections)\n",
        "        A = torch.matmul(X, self.KP)  # A will have shape (B, T, output_dim)\n",
        "\n",
        "        # Apply scaling to the attention scores\n",
        "        norm_A_sq = torch.norm(A, p=2, dim=-1, keepdim=True) ** 2  # (B, T, 1)\n",
        "        S = (A * self.scale_factor) / (norm_A_sq + 1e-6)  # Apply the scaling factor\n",
        "\n",
        "        # Apply GeLU non-linearity to the attention scores\n",
        "        S = F.gelu(S)  # GeLU activation\n",
        "\n",
        "        # Use value projection to compute the final output\n",
        "        O = torch.matmul(S, self.VP.T)  # Output will have shape (B, T, output_dim)\n",
        "\n",
        "        return O\n",
        "\n",
        "    def __call__(self, X):\n",
        "        return self.forward(X)\n",
        "\n",
        "\n",
        "# Test the function\n",
        "torch.manual_seed(1337)\n",
        "B, T, C = 4, 8, 32  # batch, time, channels\n",
        "\n",
        "x = torch.randn(B, T, C)\n",
        "output_dim = 16  # Output dimension\n",
        "\n",
        "# Call pattention for key, query, and value\n",
        "key = pattention(C, output_dim)  # k shape: (B, T, output_dim)\n",
        "query = pattention(C, output_dim)  # q shape: (B, T, output_dim)\n",
        "value = pattention(C, output_dim)  # v shape: (B, T, output_dim)\n",
        "\n",
        "k = key(x)   # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "# wei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "\n",
        "# tril = torch.tril(torch.ones(T, T)).to(x.device)  # Ensure the mask is on the same device as input tensor\n",
        "# wei = wei.masked_fill(tril == 0, float('-inf'))  # Apply the mask\n",
        "# wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "# v = value(x)\n",
        "# out = wei @ v\n",
        "\n",
        "print(\"Key shape:\", k.shape)\n",
        "print(\"Query shape:\", q.shape)\n",
        "# print(\"Value shape:\", v.shape)\n",
        "# print(\"Output shape:\", out.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2cIoM1nZnNT",
        "outputId": "32b4162d-6005-474d-f6e9-8e7e66fd1b7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Key shape: torch.Size([50, 8, 32])\n",
            "Query shape: torch.Size([50, 8, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UwfBHLO-apAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pattention"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "rRWqxASNZWwA",
        "outputId": "ca39f7ca-29af-48e2-bf91-83351bdfb6c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "__main__.pattention"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pattention</b><br/>def __call__(X)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\"></a>&lt;no docstring&gt;</pre></div>"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SXxfrJDCZWrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zLGdE1SFZWnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Full finished code, for reference\n",
        "\n",
        "You may want to refer directly to the git repo instead though."
      ],
      "metadata": {
        "id": "ZcvKeBXoZFOY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fHFoa-wmDpHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x92Wc4ebFQqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Token-former working also showing the hidden layer, the shape of Key and Value of Pattention"
      ],
      "metadata": {
        "id": "80P2vSVFHeFU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Pokemon text"
      ],
      "metadata": {
        "id": "xhn1kfkNDv6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# URL of the file to download\n",
        "url = \"https://raw.githubusercontent.com/Rajarshi12321/gpt-dev-token-former-/refs/heads/main/pokemon.txt\"\n",
        "\n",
        "# Path to save the downloaded file\n",
        "file_path = \"pokemon.txt\"\n",
        "\n",
        "# Download the file\n",
        "response = requests.get(url)\n",
        "if response.status_code == 200:\n",
        "    with open(file_path, \"wb\") as file:\n",
        "        file.write(response.content)\n",
        "    print(f\"File downloaded and saved as '{file_path}'\")\n",
        "else:\n",
        "    print(f\"Failed to download file. Status code: {response.status_code}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uHQ_P4s4xtQ",
        "outputId": "9e457050-6364-48ad-d8ec-355f28db27a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded and saved as 'pokemon.txt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Updating Andrew Karpathy's transformer from scratch code to include the Pattention layer"
      ],
      "metadata": {
        "id": "2KLtF0wwDyr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "# max_iters = 1000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('pokemon.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(f'vocab size: {vocab_size}')\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    # print(\"X shape:\", x.shape)\n",
        "    # print(\"Y shape:\", y.shape)\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "\n",
        "class pattention(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, intermediate_dim=128, scale_factor=None, device=None):\n",
        "        \"\"\"\n",
        "        Initializes the attention mechanism with key and value projections using nn.Linear layers.\n",
        "        \"\"\"\n",
        "        super().__init__()  # Initialize nn.Module\n",
        "        self.input_dim = input_dim\n",
        "        self.intermediate_dim = intermediate_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        # Set the device\n",
        "        # self.device = device or torch.device('cpu')\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        # Initialize key and value projection layers as nn.Linear\n",
        "        self.KP = nn.Linear(input_dim, intermediate_dim, bias=False).to(self.device)  # Key projection layer\n",
        "        self.VP = nn.Linear(intermediate_dim, output_dim, bias=False).to(self.device)  # Value projection layer\n",
        "\n",
        "        # Optional scale factor (defaults to output_dim if not provided)\n",
        "        self.scale_factor = scale_factor or output_dim\n",
        "\n",
        "    def forward(self, X):\n",
        "        # Ensure input is on the same device as KP and VP\n",
        "        X = X.to(self.device)\n",
        "\n",
        "        # Compute the attention scores (dot product between input and key projections)\n",
        "        A = self.KP(X)  # A will have shape (B, T, intermediate_dim)\n",
        "        print(f\"KP Shape: {A.shape}\")\n",
        "\n",
        "\n",
        "        # Normalize and scale the attention scores\n",
        "        norm_A_sq = torch.norm(A, p=2, dim=-1, keepdim=True) ** 2\n",
        "        S = (A * self.scale_factor) / (norm_A_sq + 1e-6)\n",
        "        S = F.gelu(S)\n",
        "\n",
        "        print(f\"S Shape: {S.shape}\")\n",
        "\n",
        "\n",
        "        # Compute the output by applying the value projection to the scaled attention\n",
        "        O = self.VP(S)  # O will have shape (B, T, output_dim)\n",
        "\n",
        "        print(f\"VP Shape: {O.shape}\\n--------------------\\n\")\n",
        "\n",
        "        return O\n",
        "\n",
        "    def __call__(self, X):\n",
        "        return self.forward(X)\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = pattention(head_size, head_size)\n",
        "        self.query = pattention(head_size, head_size)\n",
        "        self.value = pattention(head_size, head_size)\n",
        "        # self.register_buffer('tril', torch.tril(torch.ones(64, 64)))  # Assuming block_size=64\n",
        "        # self.register_buffer('tril', torch.tril(torch.ones(T, T)))  # Assuming block_size=head_size\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        # print(f\"Head Input Shape: {x.shape}\")\n",
        "\n",
        "\n",
        "        k = self.key(x)   # (B, T, C)\n",
        "        q = self.query(x) # (B, T, C)\n",
        "        print(f\"Key Shape: {k.shape}, Query Shape: {q.shape}\")\n",
        "\n",
        "\n",
        "        # Compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5  # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        # Dynamically create the mask\n",
        "        tril = torch.tril(torch.ones(T, T, device=x.device))  # Ensure tril is on the same device as x\n",
        "        wei = wei.masked_fill(tril[:T, :T] == 0, float('-inf')).to(self.device)  # Ensure wei is on the correct device\n",
        "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        # print(f\"Attention Weights Shape: {wei.shape}\")\n",
        "\n",
        "        v = self.value(x)  # (B, T, C)\n",
        "        out = wei @ v  # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        print(f\"Head Output Shape: {out.shape}\")\n",
        "\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.head_size = head_size\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        # self.proj = nn.Linear(num_heads * head_size, num_heads * head_size)\n",
        "        self.proj = pattention(num_heads * head_size, num_heads * head_size)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        print(f\"MultiHeadAttention Input Shape: {x.shape}\")\n",
        "\n",
        "\n",
        "        # Split the input tensor into num_heads chunks of size head_size\n",
        "        # (B, T, C) -> (B, T, num_heads, head_size)\n",
        "        x_split = x.view(B, T, self.num_heads, self.head_size)\n",
        "        x_split = x_split.permute(2, 0, 1, 3)  # (num_heads, B, T, head_size)\n",
        "\n",
        "        # Pass each chunk into its corresponding head\n",
        "        out = torch.cat([head(x_split[i]) for i, head in enumerate(self.heads)], dim=-1)  # Concatenate along the last dimension\n",
        "\n",
        "        # Projection and dropout after concatenation\n",
        "        out = self.proj(out)  # (B, T, num_heads * head_size)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Print the input shape before passing through the feedforward network\n",
        "        # print(f\"Input shape to FeedForward: {x.shape}\")\n",
        "        # print(f\"FeedForward Input Shape: {x.shape}\")\n",
        "        x = self.net(x)\n",
        "        # print(f\"FeedForward Output Shape: {x.shape}\")\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        # print((n_head, head_size),\"let's see\")\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "         # Print the input shape before passing through the block\n",
        "        print(f\"Input shape to Block: {x.shape}\")\n",
        "\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "\n",
        "        # Print the output shape after block processing\n",
        "        print(f\"Output shape from Block: {x.shape}\")\n",
        "\n",
        "        return x\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            # print(\"X shape:\", X.shape)  # Check the shape of input batch\n",
        "            # print(\"Y shape:\", Y.shape)  # Check the shape of target batch\n",
        "            logits, loss = model(X, Y)\n",
        "\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self,n_embd):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        # print(f\"Shape after embedding: {x.shape}\")  # (B,T,C)\n",
        "\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel(n_embd = 64)\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "max_iters = 1\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "# print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KkXidljFQjU",
        "outputId": "f5bffacd-dc0f-4f64-80d6-8c55ce56306f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "step 0: train loss 4.4939, val loss 4.4881\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n",
            "Input shape to Block: torch.Size([16, 32, 64])\n",
            "MultiHeadAttention Input Shape: torch.Size([16, 32, 64])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Key Shape: torch.Size([16, 32, 16]), Query Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 16])\n",
            "--------------------\n",
            "\n",
            "Head Output Shape: torch.Size([16, 32, 16])\n",
            "KP Shape: torch.Size([16, 32, 128])\n",
            "S Shape: torch.Size([16, 32, 128])\n",
            "VP Shape: torch.Size([16, 32, 64])\n",
            "--------------------\n",
            "\n",
            "Output shape from Block: torch.Size([16, 32, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5bRAeo_xFyRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = BigramLanguageModel(n_embd = 64)\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# # create a PyTorch optimizer\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n"
      ],
      "metadata": {
        "id": "qFeqLjQyFx7Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7acdd4f-073a-4d89-8214-c39b43dd0913"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.407629 M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "lE1p6QLpFQbU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aea98f3c-305b-445d-cd12-beea9277ce24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BigramLanguageModel(\n",
              "  (token_embedding_table): Embedding(77, 64)\n",
              "  (position_embedding_table): Embedding(32, 64)\n",
              "  (blocks): Sequential(\n",
              "    (0): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-3): 4 x Head(\n",
              "            (key): pattention(\n",
              "              (KP): Linear(in_features=16, out_features=128, bias=False)\n",
              "              (VP): Linear(in_features=128, out_features=16, bias=False)\n",
              "            )\n",
              "            (query): pattention(\n",
              "              (KP): Linear(in_features=16, out_features=128, bias=False)\n",
              "              (VP): Linear(in_features=128, out_features=16, bias=False)\n",
              "            )\n",
              "            (value): pattention(\n",
              "              (KP): Linear(in_features=16, out_features=128, bias=False)\n",
              "              (VP): Linear(in_features=128, out_features=16, bias=False)\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): pattention(\n",
              "          (KP): Linear(in_features=64, out_features=128, bias=False)\n",
              "          (VP): Linear(in_features=128, out_features=64, bias=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
              "          (3): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (1): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-3): 4 x Head(\n",
              "            (key): pattention(\n",
              "              (KP): Linear(in_features=16, out_features=128, bias=False)\n",
              "              (VP): Linear(in_features=128, out_features=16, bias=False)\n",
              "            )\n",
              "            (query): pattention(\n",
              "              (KP): Linear(in_features=16, out_features=128, bias=False)\n",
              "              (VP): Linear(in_features=128, out_features=16, bias=False)\n",
              "            )\n",
              "            (value): pattention(\n",
              "              (KP): Linear(in_features=16, out_features=128, bias=False)\n",
              "              (VP): Linear(in_features=128, out_features=16, bias=False)\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): pattention(\n",
              "          (KP): Linear(in_features=64, out_features=128, bias=False)\n",
              "          (VP): Linear(in_features=128, out_features=64, bias=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
              "          (3): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (2): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-3): 4 x Head(\n",
              "            (key): pattention(\n",
              "              (KP): Linear(in_features=16, out_features=128, bias=False)\n",
              "              (VP): Linear(in_features=128, out_features=16, bias=False)\n",
              "            )\n",
              "            (query): pattention(\n",
              "              (KP): Linear(in_features=16, out_features=128, bias=False)\n",
              "              (VP): Linear(in_features=128, out_features=16, bias=False)\n",
              "            )\n",
              "            (value): pattention(\n",
              "              (KP): Linear(in_features=16, out_features=128, bias=False)\n",
              "              (VP): Linear(in_features=128, out_features=16, bias=False)\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): pattention(\n",
              "          (KP): Linear(in_features=64, out_features=128, bias=False)\n",
              "          (VP): Linear(in_features=128, out_features=64, bias=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
              "          (3): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (3): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-3): 4 x Head(\n",
              "            (key): pattention(\n",
              "              (KP): Linear(in_features=16, out_features=128, bias=False)\n",
              "              (VP): Linear(in_features=128, out_features=16, bias=False)\n",
              "            )\n",
              "            (query): pattention(\n",
              "              (KP): Linear(in_features=16, out_features=128, bias=False)\n",
              "              (VP): Linear(in_features=128, out_features=16, bias=False)\n",
              "            )\n",
              "            (value): pattention(\n",
              "              (KP): Linear(in_features=16, out_features=128, bias=False)\n",
              "              (VP): Linear(in_features=128, out_features=16, bias=False)\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): pattention(\n",
              "          (KP): Linear(in_features=64, out_features=128, bias=False)\n",
              "          (VP): Linear(in_features=128, out_features=64, bias=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
              "          (3): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (ln_f): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "  (lm_head): Linear(in_features=64, out_features=77, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Save the model state dictionary\n",
        "torch.save(model.state_dict(), \"pretrained_model.pth\")\n",
        "\n",
        "print(\"Model saved as pretrained_model.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oddOkvoq9Mxl",
        "outputId": "0d2f11f6-1ed5-4dc4-f0fa-5a28adfcc305"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved as pretrained_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PJW9bcyG9P58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eSoSUBol9TR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code to save extend the Key, Value and Query matrices while also keeping the old weights of the base model"
      ],
      "metadata": {
        "id": "MJdgWy1eIi9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def extend_linear_layer_for_KP(layer, new_out_features):\n",
        "    \"\"\"\n",
        "    Extends the given linear layer to have new_out_features while preserving existing weights.\n",
        "\n",
        "    Args:\n",
        "        layer (nn.Linear): The original linear layer.\n",
        "        new_out_features (int): The desired number of output features.\n",
        "\n",
        "    Returns:\n",
        "        nn.Linear: The extended linear layer.\n",
        "    \"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "    old_out_features, in_features = layer.weight.shape\n",
        "    print(f\"old layer KP: {layer.weight.shape}\")\n",
        "\n",
        "    if new_out_features <= old_out_features:\n",
        "        raise ValueError(\"new_out_features must be greater than the current out_features.\")\n",
        "\n",
        "    in_features = layer.in_features\n",
        "    old_out_features = layer.out_features\n",
        "\n",
        "    # Create a new linear layer with the new output features\n",
        "    new_layer = nn.Linear(in_features, new_out_features, bias=False).to(device)\n",
        "\n",
        "    print(f\"new layer KP: {new_layer.weight.shape}\")\n",
        "    # print(f\"new layer VP: {new_layer.weight.shape}\")\n",
        "\n",
        "    # Preserve the original weights by copying them over\n",
        "    with torch.no_grad():\n",
        "        new_layer.weight.data[:old_out_features, :] = layer.weight.data\n",
        "\n",
        "        # Initialize the new rows (for the added output features) with random values or zeros\n",
        "        new_layer.weight.data[old_out_features:, :] = torch.randn(\n",
        "            new_out_features - old_out_features, in_features).to(device)\n",
        "\n",
        "\n",
        "    return new_layer\n",
        "\n",
        "\n",
        "\n",
        "def extend_linear_layer_for_VP(layer, new_in_features):\n",
        "    \"\"\"\n",
        "    Extends the given linear layer to have new input features while preserving existing weights.\n",
        "\n",
        "    Args:\n",
        "        layer (nn.Linear): The original linear layer.\n",
        "        new_in_features (int): The desired number of input features.\n",
        "\n",
        "    Returns:\n",
        "        nn.Linear: The extended linear layer.\n",
        "    \"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    old_in_features, old_out_features = layer.weight.shape\n",
        "    # print(layer.weight.shape,old_in_features, old_out_features,new_in_features )\n",
        "    # print(old_out_features <= new_in_features,old_out_features,new_in_features,\"HII\")\n",
        "    if new_in_features <= old_out_features:\n",
        "        raise ValueError(\"new_in_features must be greater than the current in_features.\")\n",
        "\n",
        "    # Create a new linear layer with the new input features and the same output features\n",
        "    new_layer = nn.Linear(new_in_features, old_in_features, bias=False).to(device)\n",
        "\n",
        "    print(f\"old layer VP: {layer.weight.shape}\")\n",
        "    print(f\"New layer VP: {new_layer.weight.shape}\")\n",
        "\n",
        "    # print(f\"New layer KP: {new_layer.weight.shape}\")\n",
        "    # print(f\"OLD layer KP: {layer.weight.shape}\")\n",
        "\n",
        "    # Preserve the original weights by copying them over to the new layer\n",
        "    with torch.no_grad():\n",
        "        # Copy the old weight values into the new layer\n",
        "\n",
        "        new_layer.weight.data[:, :old_out_features] = layer.weight.data\n",
        "\n",
        "        # Optionally initialize the weights for the new input features (for the added columns)\n",
        "        # Initialize the new columns (for the additional input features) with zeros or random values\n",
        "        new_layer.weight.data[:, old_out_features:] = torch.randn(\n",
        "            old_in_features, new_in_features - old_out_features).to(device)\n",
        "\n",
        "\n",
        "    return new_layer\n",
        "\n",
        "def extend_attention_dimensions(model, new_features_dim):\n",
        "    \"\"\"\n",
        "    Extends key, query, value layers in the model to new_out_features while preserving weights.\n",
        "    \"\"\"\n",
        "    new_in_features = new_out_features = new_features_dim\n",
        "    for block in model.blocks:\n",
        "        for head in block.sa.heads:\n",
        "\n",
        "            # print(head.key.KP.weight.shape)\n",
        "            # print(head.key.VP.weight.shape,\"VPP\")\n",
        "            head.key.KP = extend_linear_layer_for_KP(head.key.KP, new_out_features)\n",
        "\n",
        "            head.key.VP = extend_linear_layer_for_VP(head.key.VP, new_in_features)\n",
        "            # return\n",
        "\n",
        "            head.query.KP = extend_linear_layer_for_KP(head.query.KP, new_out_features)\n",
        "            head.query.VP = extend_linear_layer_for_VP(head.query.VP, new_in_features)\n",
        "\n",
        "            head.value.KP = extend_linear_layer_for_KP(head.value.KP, new_out_features)\n",
        "            head.value.VP = extend_linear_layer_for_VP(head.value.VP, new_in_features)\n",
        "\n",
        "        # Update proj layer to match the concatenated output of the new heads\n",
        "        num_heads = len(block.sa.heads)\n",
        "        block.sa.proj.KP = extend_linear_layer_for_KP(block.sa.proj.KP, num_heads * new_out_features)\n",
        "        block.sa.proj.VP = extend_linear_layer_for_VP(block.sa.proj.VP, num_heads * new_out_features)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def extend_model(model, new_out_features):\n",
        "    \"\"\"\n",
        "    Extend the entire model: token embeddings, position embeddings, attention layers,\n",
        "    feedforward layers, and the final output layer (lm_head and ln_f) to the new output size,\n",
        "    while preserving the pre-trained weights.\n",
        "    \"\"\"\n",
        "    # Extend the token embedding layer\n",
        "    # model.token_embedding_table = nn.Embedding(77, 4 * new_out_features)\n",
        "\n",
        "    # Extend the position embedding layer\n",
        "    # model.position_embedding_table = nn.Embedding(32, 4 * new_out_features)\n",
        "\n",
        "    # Extend the multi-head attention layers\n",
        "    model = extend_attention_dimensions(model, new_out_features)\n",
        "\n",
        "\n",
        "    return model\n",
        "\n",
        "# Example usage:\n",
        "\n",
        "# Load the state dictionary\n",
        "state_dict = torch.load('pretrained_model.pth',weights_only=True)\n",
        "\n",
        "model = BigramLanguageModel(n_embd = 64)\n",
        "\n",
        "# Apply the state dictionary to the model\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "# Example usage:\n",
        "new_out_features = 256  # Desired new output size for the model\n",
        "# new_out_features = 10  # Desired new output size for the model\n",
        "model = extend_model(model, new_out_features)\n",
        "\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsmPbd_AE8M7",
        "outputId": "3c03219a-03fd-44fd-f162-33331ff6e4ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "old layer KP: torch.Size([128, 16])\n",
            "new layer KP: torch.Size([256, 16])\n",
            "old layer VP: torch.Size([16, 128])\n",
            "New layer VP: torch.Size([16, 256])\n",
            "old layer KP: torch.Size([128, 16])\n",
            "new layer KP: torch.Size([256, 16])\n",
            "old layer VP: torch.Size([16, 128])\n",
            "New layer VP: torch.Size([16, 256])\n",
            "old layer KP: torch.Size([128, 16])\n",
            "new layer KP: torch.Size([256, 16])\n",
            "old layer VP: torch.Size([16, 128])\n",
            "New layer VP: torch.Size([16, 256])\n",
            "old layer KP: torch.Size([128, 16])\n",
            "new layer KP: torch.Size([256, 16])\n",
            "old layer VP: torch.Size([16, 128])\n",
            "New layer VP: torch.Size([16, 256])\n",
            "old layer KP: torch.Size([128, 16])\n",
            "new layer KP: torch.Size([256, 16])\n",
            "old layer VP: torch.Size([16, 128])\n",
            "New layer VP: torch.Size([16, 256])\n",
            "old layer KP: torch.Size([128, 16])\n",
            "new layer KP: torch.Size([256, 16])\n",
            "old layer VP: torch.Size([16, 128])\n",
            "New layer VP: torch.Size([16, 256])\n",
            "old layer KP: torch.Size([128, 16])\n",
            "new layer KP: torch.Size([256, 16])\n",
            "old layer VP: torch.Size([16, 128])\n",
            "New layer VP: torch.Size([16, 256])\n",
            "old layer KP: torch.Size([128, 16])\n",
            "new layer KP: torch.Size([256, 16])\n",
            "old layer VP: torch.Size([16, 128])\n",
            "New layer VP: torch.Size([16, 256])\n",
            "old layer KP: torch.Size([128, 16])\n",
            "new layer KP: torch.Size([256, 16])\n",
            "old layer VP: torch.Size([16, 128])\n",
            "New layer VP: torch.Size([16, 256])\n",
            "old layer KP: torch.Size([128, 16])\n",
            "new layer KP: torch.Size([256, 16])\n",
            "old layer VP: torch.Size([16, 128])\n",
            "New layer VP: torch.Size([16, 256])\n",
            "old layer KP: torch.Size([128, 16])\n",
            "new layer KP: torch.Size([256, 16])\n",
            "old layer VP: torch.Size([16, 128])\n",
            "New layer VP: torch.Size([16, 256])\n",
            "old layer KP: torch.Size([128, 16])\n",
            "new layer KP: torch.Size([256, 16])\n",
            "old layer VP: torch.Size([16, 128])\n",
            "New layer VP: torch.Size([16, 256])\n",
            "old layer KP: torch.Size([128, 64])\n",
            "new layer KP: torch.Size([1024, 64])\n",
            "old layer VP: torch.Size([64, 128])\n",
            "New layer VP: torch.Size([64, 1024])\n",
            "old layer KP: torch.Size([128, 16])\n",
            "new layer KP: torch.Size([256, 16])\n",
            "old layer VP: torch.Size([16, 128])\n",
            "New layer VP: torch.Size([16, 256])\n",
            "old layer KP: torch.Size([128, 16])\n",
            "new layer KP: torch.Size([256, 16])\n",
            "old layer VP: torch.Size([16, 128])\n",
            "New layer VP: torch.Size([16, 256])\n",
            "old layer KP: torch.Size([128, 16])\n",
            "new layer KP: torch.Size([256, 16])\n",
            "old layer VP: torch.Size([16, 128])\n",
            "New layer VP: torch.Size([16, 256])\n",
            "old layer KP: torch.Size([128, 16])\n",
            "new layer KP: torch.Size([256, 16])\n",
            "old layer VP: torch.Size([16, 128])\n",
            "New layer VP: torch.Size([16, 256])\n",
            "old layer KP: torch.Size([128, 16])\n",
            "new layer KP: torch.Size([256, 16])\n",
            "old layer VP: torch.Size([16, 128])\n",
            "New layer VP: torch.Size([16, 256])\n",
            "old layer KP: torch.Size([128, 16])\n",
            "new layer KP: torch.Size([256, 16])\n",
            "old layer VP: torch.Size([16, 128])\n",
            "New layer VP: torch.Size([16, 256])\n",
            "old layer KP: torch.Size([128, 16])\n",
            "new layer KP: torch.Size([256, 16])\n",
            "old layer VP: torch.Size([16, 128])\n",
            "New layer VP: torch.Size([16, 256])\n",
            "old layer KP: torch.Size([128, 16])\n",
            "new layer KP: torch.Size([256, 16])\n",
            "old layer VP: torch.Size([16, 128])\n",
            "New layer VP: torch.Size([16, 256])\n",
            "old layer KP: torch.Size([128, 16])\n",
            "new layer KP: torch.Size([256, 16])\n",
            "old layer VP: torch.Size([16, 128])\n",
            "New layer VP: torch.Size([16, 256])\n",
            "old layer KP: torch.Size([128, 16])\n",
            "new layer KP: torch.Size([256, 16])\n",
            "old layer VP: torch.Size([16, 128])\n",
            "New layer VP: torch.Size([16, 256])\n",
            "old layer KP: torch.Size([128, 16])\n",
            "new layer KP: torch.Size([256, 16])\n",
            "old layer VP: torch.Size([16, 128])\n",
            "New layer VP: torch.Size([16, 256])\n",
            "old layer KP: torch.Size([128, 16])\n",
            "new layer KP: torch.Size([256, 16])\n",
            "old layer VP: torch.Size([16, 128])\n",
            "New layer VP: torch.Size([16, 256])\n",
            "old layer KP: torch.Size([128, 64])\n",
            "new layer KP: torch.Size([1024, 64])\n",
            "old layer VP: torch.Size([64, 128])\n",
            "New layer VP: torch.Size([64, 1024])\n",
            "old layer KP: torch.Size([128, 16])\n",
            "new layer KP: torch.Size([256, 16])\n",
            "old layer VP: torch.Size([16, 128])\n",
            "New layer VP: torch.Size([16, 256])\n",
            "old layer KP: torch.Size([128, 16])\n",
            "new layer KP: torch.Size([256, 16])\n",
            "old layer VP: torch.Size([16, 128])\n",
            "New layer VP: torch.Size([16, 256])\n",
            "old layer KP: torch.Size([128, 16])\n",
            "new layer KP: torch.Size([256, 16])\n",
            "old layer VP: torch.Size([16, 128])\n",
            "New layer VP: torch.Size([16, 256])\n",
            "old layer KP: torch.Size([128, 16])\n",
            "new layer KP: torch.Size([256, 16])\n",
            "old layer VP: torch.Size([16, 128])\n",
            "New layer VP: torch.Size([16, 256])\n",
            "old layer KP: torch.Size([128, 16])\n",
            "new layer KP: torch.Size([256, 16])\n",
            "old layer VP: torch.Size([16, 128])\n",
            "New layer VP: torch.Size([16, 256])\n",
            "old layer KP: torch.Size([128, 16])\n",
            "new layer KP: torch.Size([256, 16])\n",
            "old layer VP: torch.Size([16, 128])\n",
            "New layer VP: torch.Size([16, 256])\n",
            "old layer KP: torch.Size([128, 16])\n",
            "new layer KP: torch.Size([256, 16])\n",
            "old layer VP: torch.Size([16, 128])\n",
            "New layer VP: torch.Size([16, 256])\n",
            "old layer KP: torch.Size([128, 16])\n",
            "new layer KP: torch.Size([256, 16])\n",
            "old layer VP: torch.Size([16, 128])\n",
            "New layer VP: torch.Size([16, 256])\n",
            "old layer KP: torch.Size([128, 16])\n",
            "new layer KP: torch.Size([256, 16])\n",
            "old layer VP: torch.Size([16, 128])\n",
            "New layer VP: torch.Size([16, 256])\n",
            "old layer KP: torch.Size([128, 16])\n",
            "new layer KP: torch.Size([256, 16])\n",
            "old layer VP: torch.Size([16, 128])\n",
            "New layer VP: torch.Size([16, 256])\n",
            "old layer KP: torch.Size([128, 16])\n",
            "new layer KP: torch.Size([256, 16])\n",
            "old layer VP: torch.Size([16, 128])\n",
            "New layer VP: torch.Size([16, 256])\n",
            "old layer KP: torch.Size([128, 16])\n",
            "new layer KP: torch.Size([256, 16])\n",
            "old layer VP: torch.Size([16, 128])\n",
            "New layer VP: torch.Size([16, 256])\n",
            "old layer KP: torch.Size([128, 64])\n",
            "new layer KP: torch.Size([1024, 64])\n",
            "old layer VP: torch.Size([64, 128])\n",
            "New layer VP: torch.Size([64, 1024])\n",
            "old layer KP: torch.Size([128, 16])\n",
            "new layer KP: torch.Size([256, 16])\n",
            "old layer VP: torch.Size([16, 128])\n",
            "New layer VP: torch.Size([16, 256])\n",
            "old layer KP: torch.Size([128, 16])\n",
            "new layer KP: torch.Size([256, 16])\n",
            "old layer VP: torch.Size([16, 128])\n",
            "New layer VP: torch.Size([16, 256])\n",
            "old layer KP: torch.Size([128, 16])\n",
            "new layer KP: torch.Size([256, 16])\n",
            "old layer VP: torch.Size([16, 128])\n",
            "New layer VP: torch.Size([16, 256])\n",
            "old layer KP: torch.Size([128, 16])\n",
            "new layer KP: torch.Size([256, 16])\n",
            "old layer VP: torch.Size([16, 128])\n",
            "New layer VP: torch.Size([16, 256])\n",
            "old layer KP: torch.Size([128, 16])\n",
            "new layer KP: torch.Size([256, 16])\n",
            "old layer VP: torch.Size([16, 128])\n",
            "New layer VP: torch.Size([16, 256])\n",
            "old layer KP: torch.Size([128, 16])\n",
            "new layer KP: torch.Size([256, 16])\n",
            "old layer VP: torch.Size([16, 128])\n",
            "New layer VP: torch.Size([16, 256])\n",
            "old layer KP: torch.Size([128, 16])\n",
            "new layer KP: torch.Size([256, 16])\n",
            "old layer VP: torch.Size([16, 128])\n",
            "New layer VP: torch.Size([16, 256])\n",
            "old layer KP: torch.Size([128, 16])\n",
            "new layer KP: torch.Size([256, 16])\n",
            "old layer VP: torch.Size([16, 128])\n",
            "New layer VP: torch.Size([16, 256])\n",
            "old layer KP: torch.Size([128, 16])\n",
            "new layer KP: torch.Size([256, 16])\n",
            "old layer VP: torch.Size([16, 128])\n",
            "New layer VP: torch.Size([16, 256])\n",
            "old layer KP: torch.Size([128, 16])\n",
            "new layer KP: torch.Size([256, 16])\n",
            "old layer VP: torch.Size([16, 128])\n",
            "New layer VP: torch.Size([16, 256])\n",
            "old layer KP: torch.Size([128, 16])\n",
            "new layer KP: torch.Size([256, 16])\n",
            "old layer VP: torch.Size([16, 128])\n",
            "New layer VP: torch.Size([16, 256])\n",
            "old layer KP: torch.Size([128, 16])\n",
            "new layer KP: torch.Size([256, 16])\n",
            "old layer VP: torch.Size([16, 128])\n",
            "New layer VP: torch.Size([16, 256])\n",
            "old layer KP: torch.Size([128, 64])\n",
            "new layer KP: torch.Size([1024, 64])\n",
            "old layer VP: torch.Size([64, 128])\n",
            "New layer VP: torch.Size([64, 1024])\n",
            "BigramLanguageModel(\n",
            "  (token_embedding_table): Embedding(77, 64)\n",
            "  (position_embedding_table): Embedding(32, 64)\n",
            "  (blocks): Sequential(\n",
            "    (0): Block(\n",
            "      (sa): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-3): 4 x Head(\n",
            "            (key): pattention(\n",
            "              (KP): Linear(in_features=16, out_features=256, bias=False)\n",
            "              (VP): Linear(in_features=256, out_features=16, bias=False)\n",
            "            )\n",
            "            (query): pattention(\n",
            "              (KP): Linear(in_features=16, out_features=256, bias=False)\n",
            "              (VP): Linear(in_features=256, out_features=16, bias=False)\n",
            "            )\n",
            "            (value): pattention(\n",
            "              (KP): Linear(in_features=16, out_features=256, bias=False)\n",
            "              (VP): Linear(in_features=256, out_features=16, bias=False)\n",
            "            )\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (proj): pattention(\n",
            "          (KP): Linear(in_features=64, out_features=1024, bias=False)\n",
            "          (VP): Linear(in_features=1024, out_features=64, bias=False)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (ffwd): FeedFoward(\n",
            "        (net): Sequential(\n",
            "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
            "          (3): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (1): Block(\n",
            "      (sa): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-3): 4 x Head(\n",
            "            (key): pattention(\n",
            "              (KP): Linear(in_features=16, out_features=256, bias=False)\n",
            "              (VP): Linear(in_features=256, out_features=16, bias=False)\n",
            "            )\n",
            "            (query): pattention(\n",
            "              (KP): Linear(in_features=16, out_features=256, bias=False)\n",
            "              (VP): Linear(in_features=256, out_features=16, bias=False)\n",
            "            )\n",
            "            (value): pattention(\n",
            "              (KP): Linear(in_features=16, out_features=256, bias=False)\n",
            "              (VP): Linear(in_features=256, out_features=16, bias=False)\n",
            "            )\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (proj): pattention(\n",
            "          (KP): Linear(in_features=64, out_features=1024, bias=False)\n",
            "          (VP): Linear(in_features=1024, out_features=64, bias=False)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (ffwd): FeedFoward(\n",
            "        (net): Sequential(\n",
            "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
            "          (3): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (2): Block(\n",
            "      (sa): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-3): 4 x Head(\n",
            "            (key): pattention(\n",
            "              (KP): Linear(in_features=16, out_features=256, bias=False)\n",
            "              (VP): Linear(in_features=256, out_features=16, bias=False)\n",
            "            )\n",
            "            (query): pattention(\n",
            "              (KP): Linear(in_features=16, out_features=256, bias=False)\n",
            "              (VP): Linear(in_features=256, out_features=16, bias=False)\n",
            "            )\n",
            "            (value): pattention(\n",
            "              (KP): Linear(in_features=16, out_features=256, bias=False)\n",
            "              (VP): Linear(in_features=256, out_features=16, bias=False)\n",
            "            )\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (proj): pattention(\n",
            "          (KP): Linear(in_features=64, out_features=1024, bias=False)\n",
            "          (VP): Linear(in_features=1024, out_features=64, bias=False)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (ffwd): FeedFoward(\n",
            "        (net): Sequential(\n",
            "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
            "          (3): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (3): Block(\n",
            "      (sa): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-3): 4 x Head(\n",
            "            (key): pattention(\n",
            "              (KP): Linear(in_features=16, out_features=256, bias=False)\n",
            "              (VP): Linear(in_features=256, out_features=16, bias=False)\n",
            "            )\n",
            "            (query): pattention(\n",
            "              (KP): Linear(in_features=16, out_features=256, bias=False)\n",
            "              (VP): Linear(in_features=256, out_features=16, bias=False)\n",
            "            )\n",
            "            (value): pattention(\n",
            "              (KP): Linear(in_features=16, out_features=256, bias=False)\n",
            "              (VP): Linear(in_features=256, out_features=16, bias=False)\n",
            "            )\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (proj): pattention(\n",
            "          (KP): Linear(in_features=64, out_features=1024, bias=False)\n",
            "          (VP): Linear(in_features=1024, out_features=64, bias=False)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (ffwd): FeedFoward(\n",
            "        (net): Sequential(\n",
            "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
            "          (3): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (ln_f): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "  (lm_head): Linear(in_features=64, out_features=77, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P8KF9yhEnQC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UsPTCZjUnP-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sized increased from 0.407629 M parameters ---> 1.062989 M parameters, by just updating the pattention layer Key and Value matrices. It don't even change the whole model architecture in terms of Input and Output dimensions of other components since the Pattention kind of acts as a hidden layer in each head in my representation"
      ],
      "metadata": {
        "id": "_2lzqTSvESn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6ozIDbpNkn9",
        "outputId": "a0aa8731-962c-48fa-df5a-93e4d7404f51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.062989 M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neDyeJxPNkiv",
        "outputId": "52aa207b-0fcf-4bf8-bb75-ac037d496738"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BigramLanguageModel(\n",
              "  (token_embedding_table): Embedding(77, 64)\n",
              "  (position_embedding_table): Embedding(32, 64)\n",
              "  (blocks): Sequential(\n",
              "    (0): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-3): 4 x Head(\n",
              "            (key): pattention(\n",
              "              (KP): Linear(in_features=16, out_features=256, bias=False)\n",
              "              (VP): Linear(in_features=256, out_features=16, bias=False)\n",
              "            )\n",
              "            (query): pattention(\n",
              "              (KP): Linear(in_features=16, out_features=256, bias=False)\n",
              "              (VP): Linear(in_features=256, out_features=16, bias=False)\n",
              "            )\n",
              "            (value): pattention(\n",
              "              (KP): Linear(in_features=16, out_features=256, bias=False)\n",
              "              (VP): Linear(in_features=256, out_features=16, bias=False)\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): pattention(\n",
              "          (KP): Linear(in_features=64, out_features=1024, bias=False)\n",
              "          (VP): Linear(in_features=1024, out_features=64, bias=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
              "          (3): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (1): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-3): 4 x Head(\n",
              "            (key): pattention(\n",
              "              (KP): Linear(in_features=16, out_features=256, bias=False)\n",
              "              (VP): Linear(in_features=256, out_features=16, bias=False)\n",
              "            )\n",
              "            (query): pattention(\n",
              "              (KP): Linear(in_features=16, out_features=256, bias=False)\n",
              "              (VP): Linear(in_features=256, out_features=16, bias=False)\n",
              "            )\n",
              "            (value): pattention(\n",
              "              (KP): Linear(in_features=16, out_features=256, bias=False)\n",
              "              (VP): Linear(in_features=256, out_features=16, bias=False)\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): pattention(\n",
              "          (KP): Linear(in_features=64, out_features=1024, bias=False)\n",
              "          (VP): Linear(in_features=1024, out_features=64, bias=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
              "          (3): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (2): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-3): 4 x Head(\n",
              "            (key): pattention(\n",
              "              (KP): Linear(in_features=16, out_features=256, bias=False)\n",
              "              (VP): Linear(in_features=256, out_features=16, bias=False)\n",
              "            )\n",
              "            (query): pattention(\n",
              "              (KP): Linear(in_features=16, out_features=256, bias=False)\n",
              "              (VP): Linear(in_features=256, out_features=16, bias=False)\n",
              "            )\n",
              "            (value): pattention(\n",
              "              (KP): Linear(in_features=16, out_features=256, bias=False)\n",
              "              (VP): Linear(in_features=256, out_features=16, bias=False)\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): pattention(\n",
              "          (KP): Linear(in_features=64, out_features=1024, bias=False)\n",
              "          (VP): Linear(in_features=1024, out_features=64, bias=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
              "          (3): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (3): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-3): 4 x Head(\n",
              "            (key): pattention(\n",
              "              (KP): Linear(in_features=16, out_features=256, bias=False)\n",
              "              (VP): Linear(in_features=256, out_features=16, bias=False)\n",
              "            )\n",
              "            (query): pattention(\n",
              "              (KP): Linear(in_features=16, out_features=256, bias=False)\n",
              "              (VP): Linear(in_features=256, out_features=16, bias=False)\n",
              "            )\n",
              "            (value): pattention(\n",
              "              (KP): Linear(in_features=16, out_features=256, bias=False)\n",
              "              (VP): Linear(in_features=256, out_features=16, bias=False)\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): pattention(\n",
              "          (KP): Linear(in_features=64, out_features=1024, bias=False)\n",
              "          (VP): Linear(in_features=1024, out_features=64, bias=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
              "          (3): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (ln_f): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "  (lm_head): Linear(in_features=64, out_features=77, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now putting the code to test, if sequential training with expanding Value and Key matrices in Pattention makes much difference in the model the or not"
      ],
      "metadata": {
        "id": "S83vAHWJFGKo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reinitiating the Model to remove print statements  showing the shape of the model layers"
      ],
      "metadata": {
        "id": "bnWXyU_lIm8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "# max_iters = 1000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('pokemon.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(f'vocab size: {vocab_size}')\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    # print(\"X shape:\", x.shape)\n",
        "    # print(\"Y shape:\", y.shape)\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "\n",
        "class pattention(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, intermediate_dim=128, scale_factor=None, device=None):\n",
        "        \"\"\"\n",
        "        Initializes the attention mechanism with key and value projections using nn.Linear layers.\n",
        "        \"\"\"\n",
        "        super().__init__()  # Initialize nn.Module\n",
        "        self.input_dim = input_dim\n",
        "        self.intermediate_dim = intermediate_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        # Set the device\n",
        "        # self.device = device or torch.device('cpu')\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        # Initialize key and value projection layers as nn.Linear\n",
        "        self.KP = nn.Linear(input_dim, intermediate_dim, bias=False).to(self.device)  # Key projection layer\n",
        "        self.VP = nn.Linear(intermediate_dim, output_dim, bias=False).to(self.device)  # Value projection layer\n",
        "\n",
        "        # Optional scale factor (defaults to output_dim if not provided)\n",
        "        self.scale_factor = scale_factor or output_dim\n",
        "\n",
        "    def forward(self, X):\n",
        "        # Ensure input is on the same device as KP and VP\n",
        "        X = X.to(self.device)\n",
        "\n",
        "        # Compute the attention scores (dot product between input and key projections)\n",
        "        A = self.KP(X)  # A will have shape (B, T, intermediate_dim)\n",
        "        # print(f\"KP Shape: {A.shape}\")\n",
        "\n",
        "\n",
        "        # Normalize and scale the attention scores\n",
        "        norm_A_sq = torch.norm(A, p=2, dim=-1, keepdim=True) ** 2\n",
        "        S = (A * self.scale_factor) / (norm_A_sq + 1e-6)\n",
        "        S = F.gelu(S)\n",
        "\n",
        "        # print(f\"S Shape: {S.shape}\")\n",
        "\n",
        "\n",
        "        # Compute the output by applying the value projection to the scaled attention\n",
        "        O = self.VP(S)  # O will have shape (B, T, output_dim)\n",
        "\n",
        "        # print(f\"VP Shape: {O.shape}\\n--------------------\\n\")\n",
        "\n",
        "        return O\n",
        "\n",
        "    def __call__(self, X):\n",
        "        return self.forward(X)\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = pattention(head_size, head_size)\n",
        "        self.query = pattention(head_size, head_size)\n",
        "        self.value = pattention(head_size, head_size)\n",
        "        # self.register_buffer('tril', torch.tril(torch.ones(64, 64)))  # Assuming block_size=64\n",
        "        # self.register_buffer('tril', torch.tril(torch.ones(T, T)))  # Assuming block_size=head_size\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        # print(f\"Head Input Shape: {x.shape}\")\n",
        "\n",
        "\n",
        "        k = self.key(x)   # (B, T, C)\n",
        "        q = self.query(x) # (B, T, C)\n",
        "        # print(f\"Key Shape: {k.shape}, Query Shape: {q.shape}\")\n",
        "\n",
        "\n",
        "        # Compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5  # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        # Dynamically create the mask\n",
        "        tril = torch.tril(torch.ones(T, T, device=x.device))  # Ensure tril is on the same device as x\n",
        "        wei = wei.masked_fill(tril[:T, :T] == 0, float('-inf')).to(self.device)  # Ensure wei is on the correct device\n",
        "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        # print(f\"Attention Weights Shape: {wei.shape}\")\n",
        "\n",
        "        v = self.value(x)  # (B, T, C)\n",
        "        out = wei @ v  # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        # print(f\"Head Output Shape: {out.shape}\")\n",
        "\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.head_size = head_size\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        # self.proj = nn.Linear(num_heads * head_size, num_heads * head_size)\n",
        "        self.proj = pattention(num_heads * head_size, num_heads * head_size)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        # print(f\"MultiHeadAttention Input Shape: {x.shape}\")\n",
        "\n",
        "\n",
        "        # Split the input tensor into num_heads chunks of size head_size\n",
        "        # (B, T, C) -> (B, T, num_heads, head_size)\n",
        "        x_split = x.view(B, T, self.num_heads, self.head_size)\n",
        "        x_split = x_split.permute(2, 0, 1, 3)  # (num_heads, B, T, head_size)\n",
        "\n",
        "        # Pass each chunk into its corresponding head\n",
        "        out = torch.cat([head(x_split[i]) for i, head in enumerate(self.heads)], dim=-1)  # Concatenate along the last dimension\n",
        "\n",
        "        # Projection and dropout after concatenation\n",
        "        out = self.proj(out)  # (B, T, num_heads * head_size)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Print the input shape before passing through the feedforward network\n",
        "        # print(f\"Input shape to FeedForward: {x.shape}\")\n",
        "        # print(f\"FeedForward Input Shape: {x.shape}\")\n",
        "        x = self.net(x)\n",
        "        # print(f\"FeedForward Output Shape: {x.shape}\")\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        # print((n_head, head_size),\"let's see\")\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "         # Print the input shape before passing through the block\n",
        "        # print(f\"Input shape to Block: {x.shape}\")\n",
        "\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "\n",
        "        # Print the output shape after block processing\n",
        "        # print(f\"Output shape from Block: {x.shape}\")\n",
        "\n",
        "        return x\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            # print(\"X shape:\", X.shape)  # Check the shape of input batch\n",
        "            # print(\"Y shape:\", Y.shape)  # Check the shape of target batch\n",
        "            logits, loss = model(X, Y)\n",
        "\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self,n_embd):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        # print(f\"Shape after embedding: {x.shape}\")  # (B,T,C)\n",
        "\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel(n_embd = 4)\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHeT-93vE8oW",
        "outputId": "a9d55448-f726-4aa2-95dc-4c8fbde2d05c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size: 77\n",
            "0.017869 M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "new_out_features = 256  # Desired new output size for the model\n",
        "# new_out_features = 10  # Desired new output size for the model\n",
        "model = extend_model(model, new_out_features)\n",
        "\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "id": "LIGOxc7onJAl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}