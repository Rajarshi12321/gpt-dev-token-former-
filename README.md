# TokenFormer with P-Attention Layer: README

## Overview

This project integrates the **P-Attention Layer** into the TokenFormer architecture, inspired by Andrej Karpathy's [NG Video Lecture project](https://github.com/karpathy/ng-video-lecture) and the [TokenFormer paper](https://arxiv.org/pdf/2410.23168). The **P-Attention Layer** introduces a scalable and efficient mechanism for attention, significantly enhancing the flexibility and efficiency of sequence modeling tasks.  

Transformers have dominated foundational models across multiple domains, but their scalability is hindered by the reliance on fixed-parameter linear projections and high retraining costs for architecture modifications. The P-Attention innovation reimagines these limitations, enabling efficient parameter scaling without retraining from scratch.

---

## Features

- **P-Attention Innovation**:
  - Treats model parameters as tokens, enabling dynamic interactions between input tokens and model parameters.
  - Replaces traditional linear projections in Transformers with token-parameter attention layers.
  - Allows incremental addition of Key-Value pairs for progressive scaling without full retraining.

- **Enhanced Architectural Flexibility**:  
  - Supports seamless scaling from 124M to 1.4B parameters while maintaining competitive performance.
  - Reduces computational costs associated with training large models from scratch.

- **Community-Oriented Design**:  
  - Implements modular components inspired by Karpathyâ€™s educational approach, facilitating experimentation and further development.

---

## Importance of P-Attention

Transformers, while excelling across various domains, are constrained by their inflexible architecture and high training costs. These limitations are primarily due to the fixed-parameter design in linear projections, necessitating full retraining for architectural changes.  

P-Attention addresses these issues by treating model parameters as tokens and leveraging token-parameter attention layers. This approach:  
1. Enables **dynamic scaling** without full retraining.  
2. Reduces **computational overhead** for scaling.  
3. Maintains or improves performance compared to models trained from scratch.  

TokenFormer thus represents a step forward in creating scalable, efficient, and flexible architectures for foundational models.  

---

## References

1. **TokenFormer Paper**: [TokenFormer: Enhancing Sequence Models with Token-Level Representations](https://arxiv.org/pdf/2410.23168)  
2. **Karpathy's NG Video Lecture**: [GitHub Repository](https://github.com/karpathy/ng-video-lecture)

---

## Acknowledgments

This project is deeply inspired by the foundational work of Andrej Karpathy and the TokenFormer team. Their innovative ideas and open-source contributions have made this implementation possible.

---

### Note to the Community

The **P-Attention Layer implementation** in this project provides a framework for scalable sequence modeling. However, **the capability of training the model by sequentially incrementing the Key and Value parameters in the P-Attention Layer has not yet been tested**.  

We encourage the community to experiment with this feature on GPUs and share their findings to further validate and refine this approach. Your contributions can help advance the development of scalable and efficient Transformer architectures! Specifically to prove it's improvement over this simplistic approach
